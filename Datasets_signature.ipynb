{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f023dea2-3a0c-4e52-b248-33ff045d6ea9",
   "metadata": {},
   "source": [
    "# Uncertanty analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10885fd0-3cf4-4958-ade7-81c84c48e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr \n",
    "import geopandas as gpd\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from oggm import utils\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from   plotly.subplots import make_subplots\n",
    "\n",
    "cl     = px.colors.qualitative.D3\n",
    "os.chdir('/home/rooda/OGGM_results/')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d893e9b0-f469-4ed9-9dc1-9ef52e8a089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RGI6_ids = gpd.read_file(\"/home/rooda/Dropbox/Patagonia/GIS South/Glaciers/RGI6_v2.shp\")\n",
    "RGI7_ids = gpd.read_file(\"/home/rooda/Dropbox/Patagonia/GIS South/Glaciers/RGI7_v2.shp\")\n",
    "RGI6_ids = RGI6_ids[RGI6_ids.area_km2 > 7][[\"RGIId\", \"Zone\", \"ID_basin\"]]\n",
    "RGI7_ids = RGI7_ids[RGI7_ids.area_km2 > 7]\n",
    "\n",
    "RGI7_ids = utils.cook_rgidf(RGI7_ids, o1_region='17', o2_region='02', bgndate= RGI7_ids.src_date, \n",
    "                            version = \"70\", assign_column_values= {'Zone' : 'Zone', 'ID_basin' : 'ID_basin'})\n",
    "\n",
    "RGI7_ids = RGI7_ids[[\"RGIId\", \"Zone\", \"ID_basin\"]]\n",
    "ids = pd.concat([RGI6_ids, RGI7_ids]).set_index(\"RGIId\")\n",
    "\n",
    "dict_zone = {1:'PPY', 2:'PCA', 3:'NPI-E', 4:'NPI-W', 5:'SPI-N', 6:'SPI-C', 7:'SPI-S', 8:'GCN', 9:'CDI'}\n",
    "ids = ids.replace({\"Zone\": dict_zone})\n",
    "\n",
    "# variables to analize\n",
    "variable = 'melt_on_glacier'\n",
    "gdirs = glob(\"/home/rooda/OGGM_results/new/*\", recursive = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d280566-50bc-46b4-873b-a88f0db3288d",
   "metadata": {},
   "source": [
    "# Peak water year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec6822-0108-4f1e-8290-814d6df07db1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak_year_dataset = []\n",
    "\n",
    "for gdir in tqdm(gdirs):\n",
    "    paths = glob(gdir + \"/run_output_*.nc\", recursive = True)\n",
    "\n",
    "    for path in tqdm(paths, leave = False):\n",
    "        model_hist   = xr.open_mfdataset(gdir + \"/run_outputs_*.nc\")[variable]\n",
    "        model_hist   = model_hist.isel(time=slice(0, -1))\n",
    "        model_future = xr.open_dataset(path)[variable]\n",
    "        model   = xr.concat([model_hist, model_future], dim = \"time\")\n",
    "        ids_subset = ids[ids.index.isin(model.rgi_id.to_pandas().tolist())]\n",
    "        \n",
    "        model = model.assign_coords(rgi_id = ids_subset.ID_basin.tolist())\n",
    "        model = model.groupby('rgi_id').sum()\n",
    "        model = model.isel(time=slice(0, -1))\n",
    "        peak_year = model.rolling(time=10, center=True).mean()\n",
    "        peak_year = peak_year.idxmax(dim = \"time\").astype(\"int16\").to_dataframe()\n",
    "        peak_year = peak_year.transpose()\n",
    "        \n",
    "        # data to save\n",
    "        peak_year.insert(0, 'Outline', os.path.basename(gdir).split(\"_\")[0])\n",
    "        peak_year.insert(1, 'Climate', os.path.basename(gdir).split(\"_\")[1])\n",
    "        peak_year.insert(2, 'Volume',  os.path.basename(gdir).split(\"_\")[2])\n",
    "        peak_year.insert(3, 'GGM',     os.path.basename(path).split(\"_\")[2])\n",
    "        peak_year.insert(4, 'SSP',     os.path.basename(path).split(\"_\")[3])\n",
    "        peak_year.insert(5, 'BCM',     os.path.basename(path).split(\"_\")[4][0:3])\n",
    "        peak_year_dataset.append(peak_year)\n",
    "        \n",
    "peak_year_dataset = pd.concat(peak_year_dataset)\n",
    "peak_year_dataset = peak_year_dataset.reset_index()\n",
    "peak_year_dataset = peak_year_dataset.drop([\"index\"], axis = 1)\n",
    "peak_year_dataset.to_csv(\"/home/rooda/Dropbox/Patagonia/MS2 Results/data_basin_peak_water_year.csv\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f1c8b-0f8a-4785-a8ef-3156acb5590e",
   "metadata": {},
   "source": [
    "# Rate of change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc77d44-5b0d-4117-a2bb-fe26d54dab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_change_dataset = []\n",
    "\n",
    "for gdir in tqdm(gdirs):\n",
    "    paths = glob(gdir + \"/run_output_*.nc\", recursive = True)\n",
    "\n",
    "    for path in tqdm(paths, leave = False): \n",
    "        model_hist   = xr.open_mfdataset(gdir + \"/run_outputs_*.nc\")[variable]\n",
    "        model_hist   = model_hist.isel(time=slice(0, -1))\n",
    "        model_future = xr.open_dataset(path)[variable]\n",
    "        model   = xr.concat([model_hist, model_future], dim = \"time\")\n",
    "        \n",
    "        ids_subset = ids[ids.index.isin(model.rgi_id.to_pandas().tolist())]\n",
    "        model  = model.assign_coords(rgi_id = ids_subset.ID_basin.tolist())     \n",
    "        model = model.groupby('rgi_id').sum()\n",
    "        model = model.isel(time=slice(0, -1))\n",
    "        \n",
    "        # search the year of the peak \n",
    "        peak_year = model.rolling(time=10, center=True).mean()\n",
    "        peak_year = peak_year.idxmax(dim = \"time\").to_numpy()\n",
    "        \n",
    "        # normalize value\n",
    "        model[\"melt_on_glacier\"] = model/model.max(dim = \"time\")\n",
    "        model = model.transpose(\"rgi_id\", \"time\") # trick to complete loop\n",
    "        \n",
    "        rate_change_i = []\n",
    "        for catchment in range(0, len(model.rgi_id)): # get the trend for each zone\n",
    "            rate_change = model[catchment].sel(time = slice(peak_year[catchment], peak_year[catchment]+30))\n",
    "            rate_change = rate_change.polyfit(dim = \"time\", deg = 1).polyfit_coefficients[0].to_numpy()\n",
    "            rate_change = rate_change * 100 * 10 # final value: %% per decade\n",
    "            rate_change_i.append(rate_change)\n",
    "        \n",
    "        rate_change = pd.DataFrame(columns =  model.rgi_id.to_numpy())\n",
    "        rate_change.loc[0] = np.array(rate_change_i)\n",
    "        \n",
    "        # data to save\n",
    "        rate_change.insert(0, 'Outline', os.path.basename(gdir).split(\"_\")[0])\n",
    "        rate_change.insert(1, 'Climate', os.path.basename(gdir).split(\"_\")[1])\n",
    "        rate_change.insert(2, 'Volume',  os.path.basename(gdir).split(\"_\")[2])\n",
    "        rate_change.insert(3, 'GGM',     os.path.basename(path).split(\"_\")[2])\n",
    "        rate_change.insert(4, 'SSP',     os.path.basename(path).split(\"_\")[3])\n",
    "        rate_change.insert(5, 'BCM',     os.path.basename(path).split(\"_\")[4][0:3])\n",
    "        rate_change_dataset.append(rate_change)\n",
    "        \n",
    "rate_change_dataset = pd.concat(rate_change_dataset)\n",
    "rate_change_dataset = rate_change_dataset.reset_index()\n",
    "rate_change_dataset = rate_change_dataset.drop([\"index\"], axis = 1)\n",
    "rate_change_dataset.to_csv(\"/home/rooda/Dropbox/Patagonia/MS2 Results/data_basin_rate_change.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8aebf-485a-4c89-92b8-5decf1183636",
   "metadata": {},
   "source": [
    "# Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570e1de-425e-47d6-aaf7-2d599f47573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_water_dataset = []\n",
    "\n",
    "for gdir in tqdm(gdirs):\n",
    "    paths = glob(gdir + \"/run_output_*.nc\", recursive = True)\n",
    "\n",
    "    for path in tqdm(paths, leave = False): \n",
    "        model_hist   = xr.open_mfdataset(gdir + \"/run_outputs_*.nc\")[variable]\n",
    "        model_hist   = model_hist.isel(time=slice(0, -1))\n",
    "        model_future = xr.open_dataset(path)[variable]\n",
    "        model   = xr.concat([model_hist, model_future], dim = \"time\")\n",
    "        \n",
    "        ids_subset = ids[ids.index.isin(model.rgi_id.to_pandas().tolist())]\n",
    "        model  = model.assign_coords(rgi_id = ids_subset.ID_basin.tolist())\n",
    "       \n",
    "        model   = model.groupby('rgi_id').sum()\n",
    "        model = model.isel(time=slice(0, -1))\n",
    "        peak_water = model.max(dim = \"time\").to_dataframe()\n",
    "        peak_water = peak_water.transpose() *1e-9\n",
    "        \n",
    "        # data to save\n",
    "        peak_water.insert(0, 'Outline', os.path.basename(gdir).split(\"_\")[0])\n",
    "        peak_water.insert(1, 'Climate', os.path.basename(gdir).split(\"_\")[1])\n",
    "        peak_water.insert(2, 'Volume',  os.path.basename(gdir).split(\"_\")[2])\n",
    "        peak_water.insert(3, 'GGM',     os.path.basename(path).split(\"_\")[2])\n",
    "        peak_water.insert(4, 'SSP',     os.path.basename(path).split(\"_\")[3])\n",
    "        peak_water.insert(5, 'BCM',     os.path.basename(path).split(\"_\")[4][0:3])\n",
    "        peak_water_dataset.append(peak_water)\n",
    "        \n",
    "peak_water_dataset = pd.concat(peak_water_dataset)\n",
    "peak_water_dataset = peak_water_dataset.reset_index()\n",
    "peak_water_dataset = peak_water_dataset.drop([\"index\"], axis = 1)\n",
    "peak_water_dataset.to_csv(\"/home/rooda/Dropbox/Patagonia/MS2 Results/data_basin_peak_water_value.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a08bd7-27be-45e4-94ab-13562c72fb0e",
   "metadata": {},
   "source": [
    "# Frecuency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dde961-5181-4fe1-b233-948a783ef1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dataset = []\n",
    "\n",
    "for gdir in tqdm(gdirs):\n",
    "    paths = glob(gdir + \"/run_output_*.nc\", recursive = True)\n",
    "\n",
    "    for path in tqdm(paths, leave = False): \n",
    "        model_hist   = xr.open_mfdataset(gdir + \"/run_outputs_*.nc\")[variable]\n",
    "        model_hist   = model_hist.isel(time=slice(0, -1))\n",
    "        model_future = xr.open_dataset(path)[variable]\n",
    "        model   = xr.concat([model_hist, model_future], dim = \"time\")\n",
    "        \n",
    "        ids_subset = ids[ids.index.isin(model.rgi_id.to_pandas().tolist())]\n",
    "        model  = model.assign_coords(rgi_id = ids_subset.ID_basin.tolist())\n",
    "\n",
    "        model = model.groupby('rgi_id').sum()\n",
    "        model = model.isel(time=slice(0, -1))\n",
    "        \n",
    "        rolling = model.rolling(time=10, center=True).mean()\n",
    "        frecuency = ((model - rolling)*1e-9)\n",
    "        frecuency = frecuency.std(dim = \"time\")\n",
    "        frecuency = frecuency.to_dataframe().transpose() \n",
    "        \n",
    "        # data to save\n",
    "        frecuency.insert(0, 'Outline', os.path.basename(gdir).split(\"_\")[0])\n",
    "        frecuency.insert(1, 'Climate', os.path.basename(gdir).split(\"_\")[1])\n",
    "        frecuency.insert(2, 'Volume',  os.path.basename(gdir).split(\"_\")[2])\n",
    "        frecuency.insert(3, 'GGM',     os.path.basename(path).split(\"_\")[2])\n",
    "        frecuency.insert(4, 'SSP',     os.path.basename(path).split(\"_\")[3])\n",
    "        frecuency.insert(5, 'BCM',     os.path.basename(path).split(\"_\")[4][0:3])\n",
    "        freq_dataset.append(frecuency)\n",
    "        \n",
    "freq_dataset = pd.concat(freq_dataset)\n",
    "freq_dataset = freq_dataset.reset_index()\n",
    "freq_dataset = freq_dataset.drop([\"index\"], axis = 1)\n",
    "freq_dataset.to_csv(\"/home/rooda/Dropbox/Patagonia/MS2 Results/data_basin_freq.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c4ab54-3684-4846-9baf-8a7444d8c26d",
   "metadata": {},
   "source": [
    "# Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f365bc-fc98-4bdd-8b72-b1541d48158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = \"melt_on_glacier_monthly\"\n",
    "duration_dataset = []\n",
    "\n",
    "for gdir in tqdm(gdirs):\n",
    "    paths = glob(gdir + \"/run_output_*.nc\", recursive = True)\n",
    "\n",
    "    for path in tqdm(paths, leave = False): \n",
    "        model_hist   = xr.open_mfdataset(gdir + \"/run_outputs_*.nc\")[variable]\n",
    "        model_hist   = model_hist.isel(time=slice(0, -1))\n",
    "        model_future = xr.open_dataset(path)[variable]\n",
    "        model   = xr.concat([model_hist, model_future], dim = \"time\")\n",
    "        \n",
    "        ids_subset = ids[ids.index.isin(model.rgi_id.to_pandas().tolist())]\n",
    "        model  = model.assign_coords(rgi_id = ids_subset.ID_basin.tolist())\n",
    "        model = model.groupby('rgi_id').sum()\n",
    "        model = model.isel(time=slice(0, -1))\n",
    "\n",
    "        duration = model.mean(dim = \"time\")*100 / model.mean(dim = \"time\").sum(dim = \"month_2d\")\n",
    "        duration = duration.sel(month_2d = [12,1,2]).sum(dim = \"month_2d\").to_dataframe().transpose() \n",
    "\n",
    "        # data to save\n",
    "        duration.insert(0, 'Outline', os.path.basename(gdir).split(\"_\")[0])\n",
    "        duration.insert(1, 'Climate', os.path.basename(gdir).split(\"_\")[1])\n",
    "        duration.insert(2, 'Volume',  os.path.basename(gdir).split(\"_\")[2])\n",
    "        duration.insert(3, 'GGM',     os.path.basename(path).split(\"_\")[2])\n",
    "        duration.insert(4, 'SSP',     os.path.basename(path).split(\"_\")[3])\n",
    "        duration.insert(5, 'BCM',     os.path.basename(path).split(\"_\")[4][0:3])\n",
    "        duration_dataset.append(duration)\n",
    "        \n",
    "duration_dataset = pd.concat(duration_dataset)\n",
    "duration_dataset = duration_dataset.reset_index()\n",
    "duration_dataset = duration_dataset.drop([\"index\"], axis = 1)\n",
    "duration_dataset.to_csv(\"/home/rooda/Dropbox/Patagonia/MS2 Results/data_basin_duration.csv\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
