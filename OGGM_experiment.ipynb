{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc6ce84-95d4-448e-a489-dbe3c4e9cb19",
   "metadata": {},
   "source": [
    "## OGGM runs with differents climates and bias correction methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf4865-6547-405c-9293-8e169f3cf18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import os \n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "\n",
    "# oggm\n",
    "from oggm import cfg, utils, workflow, graphics, tasks, shop\n",
    "from oggm.shop import gcm_climate\n",
    "from oggm.shop import millan22\n",
    "\n",
    "# climate-related\n",
    "import cftime\n",
    "import regionmask\n",
    "from xclim import sdba\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\") \n",
    "\n",
    "chunks_dict = {\"lon\": 10, \"lat\": 10, \"time\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07677b2-c29a-4e5c-8578-7cdbe19e75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all basins in 9 selected zones \n",
    "basins = gpd.read_file(\"/home/rooda/Dropbox/Patagonia/GIS South/Basins_Patagonia_all.shp\")[[\"Zone\", \"geometry\"]]\n",
    "\n",
    "# Glacier centroid location\n",
    "ids = gpd.read_file(\"/home/rooda/Dropbox/Patagonia/GIS South/Glaciers/RGI6.shp\")\n",
    "ids = gpd.GeoDataFrame(ids, geometry=gpd.points_from_xy(ids.CenLon, ids.CenLat))\n",
    "\n",
    "# Glaciers to run (filter by area)\n",
    "ids = ids[ids.Area > 10]\n",
    "\n",
    "# Select the glaciers inside (centroid) the basin (TODO: move to glacier outlet)\n",
    "ids = gpd.clip(ids, basins, keep_geom_type = False)\n",
    "ids = ids.sjoin(basins, how=\"inner\", predicate='intersects')\n",
    "ids_list = ids.RGIId\n",
    "ids_zone = ids[[\"RGIId\",\"Zone\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488588d4-5f19-4d5c-8b48-19941e5eccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.initialize(logging_level='ERROR')\n",
    "\n",
    "cfg.PARAMS['use_multiprocessing']  = True\n",
    "cfg.PARAMS['baseline_climate']     = ''\n",
    "cfg.PARAMS['prcp_scaling_factor']  = 1\n",
    "cfg.PARAMS['hydro_month_sh']       = 1\n",
    "cfg.PARAMS['hydro_month_nh']       = 1\n",
    "cfg.PARAMS['border']               = 80\n",
    "cfg.PARAMS['min_mu_star']          = 5\n",
    "cfg.PARAMS['max_mu_star']          = 600\n",
    "cfg.PARAMS['geodetic_mb_period']   = '2000-01-01_2020-01-01' \n",
    "cfg.PARAMS['store_model_geometry'] = True\n",
    "cfg.PARAMS['continue_on_error']    = True\n",
    "cfg.PARAMS['use_winter_prcp_factor']    = False\n",
    "\n",
    "inversion_by_zone = True         # A factor by region or zone\n",
    "file_id         = \"PMET\"         # Climate baseline\n",
    "bias_correction = [\"DQM\", \"MVA\"] # Bias correction method\n",
    "volume          = \"Millan22_no\"     # Reference volume dataset \n",
    "\n",
    "cfg.PATHS['working_dir']  = \"/home/rooda/OGGM_results/\" + file_id + \"_run\"\n",
    "cfg.PATHS['climate_file'] = \"/home/rooda/OGGM_results/\"+ file_id +\"_OGGM_1980_2019m.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a466c1c-2a7c-4c62-83d1-4c8d0e20162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.4/L1-L2_files/elev_bands/' # RGI 62\n",
    "gdirs = workflow.init_glacier_directories(ids_list, from_prepro_level=2, prepro_border = 80, prepro_base_url = base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc818a-c068-4c87-8a0c-705a78af9140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdirs = workflow.init_glacier_directories(ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7708aa88-e50d-4046-ac43-d420e3a13bab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rooda/miniconda3/envs/oggm/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rooda/miniconda3/envs/oggm/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rooda/miniconda3/envs/oggm/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rooda/miniconda3/envs/oggm/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/rooda/miniconda3/envs/oggm/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rooda/miniconda3/envs/oggm/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rooda/miniconda3/envs/oggm/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rooda/miniconda3/envs/oggm/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rooda/miniconda3/envs/oggm/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rooda/miniconda3/envs/oggm/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# baseline climate tasks\n",
    "baseline_period = slice(\"1980-01-01\", \"2014-12-31\") # ISIMIP3b bias adjustment protocol\n",
    "\n",
    "# prepare baseline climate for bias correction of projections \n",
    "pp_baseline  = xr.open_dataset(cfg.PATHS['climate_file'])[\"prcp\"]\n",
    "pp_baseline  = pp_baseline.sel(time=baseline_period)\n",
    "\n",
    "t2m_baseline = xr.open_dataset(cfg.PATHS['climate_file'])[\"temp\"]\n",
    "t2m_baseline = t2m_baseline.sel(time=baseline_period)\n",
    "\n",
    "# write climate file for each glacier\n",
    "workflow.execute_entity_task(tasks.process_custom_climate_data, gdirs);\n",
    "\n",
    "# calibration\n",
    "utils.get_geodetic_mb_dataframe()\n",
    "workflow.execute_entity_task(tasks.mu_star_calibration_from_geodetic_mb, gdirs); \n",
    "workflow.execute_entity_task(tasks.apparent_mb_from_any_mb, gdirs);\n",
    "\n",
    "# inversion (for all the region or by zone)\n",
    "if volume == \"Millan22\":\n",
    "    workflow.execute_entity_task(millan22.thickness_to_gdir,gdirs);\n",
    "    compile_volume = millan22.compile_millan_statistics(gdirs)\n",
    "    compile_volume = compile_volume.replace([np.inf, -np.inf], np.nan, inplace=False)\n",
    "    \n",
    "    if inversion_by_zone:\n",
    "        for zone in range(1,10):\n",
    "            ids_subset = ids_zone[ids_zone.Zone == zone].RGIId.tolist()\n",
    "            gdirs_subset = [gdir for gdir in gdirs if gdir.rgi_id in ids_subset]\n",
    "            compile_volume_subset = compile_volume[compile_volume.index.isin(ids_subset)]\n",
    "            workflow.calibrate_inversion_from_consensus(gdirs_subset, volume_m3_reference = compile_volume_subset.millan_vol_km3.sum()*1e9,\n",
    "                                                        apply_fs_on_mismatch=True, error_on_mismatch=False, filter_inversion_output=True);\n",
    "    else:\n",
    "        workflow.calibrate_inversion_from_consensus(gdirs, volume_m3_reference = compile_volume.millan_vol_km3.sum()*1e9, \n",
    "                                                    apply_fs_on_mismatch=True, error_on_mismatch=False, filter_inversion_output=True);\n",
    "\n",
    "else:\n",
    "    if inversion_by_zone:\n",
    "        for zone in range(1,10):\n",
    "            gdirs_subset = [gdir for gdir in gdirs if gdir.rgi_id in ids_zone[ids_zone.Zone == zone].RGIId.tolist()]\n",
    "            workflow.calibrate_inversion_from_consensus(gdirs_subset, apply_fs_on_mismatch=True, error_on_mismatch=False, filter_inversion_output=True);\n",
    "    else:\n",
    "        workflow.calibrate_inversion_from_consensus(gdirs, apply_fs_on_mismatch=True, error_on_mismatch=False, filter_inversion_output=True);\n",
    "\n",
    "    \n",
    "workflow.execute_entity_task(tasks.init_present_time_glacier, gdirs); # ready to use\n",
    "utils.compile_glacier_statistics(gdirs);  # compile few things\n",
    "\n",
    "file_id = \"_\" + file_id\n",
    "workflow.execute_entity_task(tasks.run_with_hydro, gdirs, \n",
    "                                run_task=tasks.run_from_climate_data,\n",
    "                                store_monthly_hydro=True,\n",
    "                                ys=2003,ye=2020,\n",
    "                                output_filesuffix=file_id);\n",
    "\n",
    "compile_run = utils.compile_run_output(gdirs, input_filesuffix=file_id)\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# future climate tasks\n",
    "future_period = slice(\"2020-01-01\", \"2099-12-31\") # period\n",
    "#days = np.array([31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31])\n",
    "\n",
    "os.chdir(\"/home/rooda/OGGM_results/Future_climate/\")\n",
    "gcm_list  = [\"ACCESS-CM2\", \"EC-Earth3\", \"INM-CM5-0\", \"KACE-1-0-G\", \"MPI-ESM1-2-HR\", \n",
    "             \"CMCC-ESM2\",  \"GFDL-ESM4\",  \"IPSL-CM6A\",  \"MIROC6\",   \"NorESM2-LM\"]\n",
    "ssp_list  = [\"ssp245\",\"ssp370\",\"ssp585\"] \n",
    "\n",
    "gcm_list  = [\"ACCESS-CM2\"]\n",
    "ssp_list  = [\"ssp245\"] \n",
    "\n",
    "for gcm in gcm_list:    \n",
    "    for ssp in ssp_list:\n",
    "        \n",
    "        pp_model_ssp = xr.open_dataset(\"PP_\" + gcm + \"_\" + ssp + \".nc\")[\"pr\"]\n",
    "        pp_model_ssp = pp_model_ssp.interp(lat = pp_baseline.lat, lon = pp_baseline.lon)\n",
    "        pp_model_ssp = pp_model_ssp.where(pp_baseline[0].notnull())\n",
    "        pp_model_ssp = pp_model_ssp.chunk(chunks_dict)\n",
    "              \n",
    "        t2m_model_ssp = xr.open_mfdataset(\"T2M_\" + gcm + \"_\" + ssp + \".nc\")[\"tas\"]\n",
    "        t2m_model_ssp = t2m_model_ssp.interp(lat = t2m_baseline.lat, lon = t2m_baseline.lon)\n",
    "        t2m_model_ssp = t2m_model_ssp.where(t2m_baseline[0].notnull())\n",
    "        t2m_model_ssp = t2m_model_ssp.chunk(chunks_dict)\n",
    "        \n",
    "        for bc in bias_correction:\n",
    "            \n",
    "            if bc == \"DQM\":\n",
    "                qdm_t2m = sdba.adjustment.QuantileDeltaMapping.train(ref = t2m_baseline, hist = t2m_model_ssp.sel(time = baseline_period), kind = \"+\", group=\"time.month\")\n",
    "                t2m_model_ssp_bc = qdm_t2m.adjust(t2m_model_ssp.sel(time = future_period), interp=\"nearest\", extrapolation=\"constant\")      \n",
    "\n",
    "                qdm_pp  = sdba.adjustment.QuantileDeltaMapping.train(ref = pp_baseline, hist = pp_model_ssp.sel(time  = baseline_period), kind = \"*\", group=\"time.month\")\n",
    "                pp_model_ssp_bc  = qdm_pp.adjust(pp_model_ssp.sel(time  = future_period), interp=\"nearest\", extrapolation=\"constant\")\n",
    "        \n",
    "            if bc == \"MVA\":\n",
    "                qdm_t2m = sdba.adjustment.Scaling.train(ref = t2m_baseline, hist = t2m_model_ssp.sel(time = baseline_period), kind = \"+\", group=\"time.month\")\n",
    "                t2m_model_ssp_bc = qdm_t2m.adjust(t2m_model_ssp.sel(time = future_period), interp=\"nearest\")      \n",
    "\n",
    "                qdm_pp  = sdba.adjustment.Scaling.train(ref = pp_baseline, hist = pp_model_ssp.sel(time  = baseline_period), kind = \"*\", group=\"time.month\")\n",
    "                pp_model_ssp_bc  = qdm_pp.adjust(pp_model_ssp.sel(time  = future_period), interp=\"nearest\")\n",
    "        \n",
    "            t2m_model_ssp_bc = t2m_model_ssp_bc.rename(\"tas\").transpose('time', 'lat', 'lon')         \n",
    "            pp_model_ssp_bc  = pp_model_ssp_bc.rename(\"pr\").transpose('time', 'lat', 'lon')\n",
    "            t2m_model_ssp_bc.to_netcdf(\"/home/rooda/OGGM_results/T2M_Projection.nc\")\n",
    "            pp_model_ssp_bc.to_netcdf(\"/home/rooda/OGGM_results/PP_Projection.nc\")\n",
    "            rid = \"_{}_{}_{}\".format(gcm, ssp, bc)\n",
    "        \n",
    "            # write future climate file for each glacier\n",
    "            workflow.execute_entity_task(gcm_climate.process_cmip_data, gdirs, filesuffix =  rid, \n",
    "                                     fpath_precip = \"/home/rooda/OGGM_results/PP_Projection.nc\", \n",
    "                                     fpath_temp = \"/home/rooda/OGGM_results/T2M_Projection.nc\", \n",
    "                                     apply_bias_correction=False);\n",
    "\n",
    "            print(gcm, ssp, datetime.now()-start)\n",
    "        \n",
    "            # run the glacier using hydro function \n",
    "            workflow.execute_entity_task(tasks.run_with_hydro, gdirs, run_task = tasks.run_from_climate_data,\n",
    "                                     climate_filename = 'gcm_data',  # use gcm_data, not climate_historical\n",
    "                                     climate_input_filesuffix = rid,  # use the chosen scenario\n",
    "                                     init_model_filesuffix = file_id,  # this is important! Start from 2020 glacier\n",
    "                                     ref_geometry_filesuffix = file_id,  # also use this as area reference\n",
    "                                     ref_area_from_y0 = True,  # and keep the same reference area as for the historical simulations\n",
    "                                     output_filesuffix = rid,  # recognize the run for later\n",
    "                                     store_monthly_hydro = True)  # add monthly diagnostics\n",
    "            compile_run = utils.compile_run_output(gdirs, input_filesuffix=rid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172e9146-2866-48be-b819-c3c69bf42fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
